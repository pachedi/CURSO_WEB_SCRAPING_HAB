list.files("./lyceemermoz.edu.ar-231729/", recursive = TRUE)
knitr::opts_chunk$set(echo = TRUE)
#install.packages("Rcrawler", dependencies = TRUE)
library(Rcrawler)
list.files("./lyceemermoz.edu.ar-231729/", recursive = TRUE)
resultados_h1 <- ContentScraper(
Url = "./lyceemermoz.edu.ar-231729/",  # Directorio donde están las páginas descargadas
CssPatterns = c("h1")  # Selecciona el patrón CSS de lo que quieres extraer
)
resultados_h1 <- ContentScraper(
Url = "./lyceemermoz.edu.ar-231729/",  # Directorio donde están las páginas descargadas
CssPatterns = c("h1")  # Selecciona el patrón CSS de lo que quieres extraer
)
resultados_h1 <- ContentScraper(
url = "./lyceemermoz.edu.ar-231729/",  # Directorio donde están las páginas descargadas
CssPatterns = c("h1")  # Selecciona el patrón CSS de lo que quieres extraer
)
resultados_h1 <- ContentScraper(
Url = "./lyceemermoz.edu.ar-231729/",  # Directorio donde están las páginas descargadas
CssPatterns = "h1"  # Selecciona el patrón CSS de lo que quieres extraer
)
enlaces <- ContentScraper(
Url = "./lyceemermoz.edu.ar-231729/",
CssPatterns = c("a"),  # Busca los elementos <a>
Attributes = c("href")  # Devuelve los atributos "href" de esos elementos
)
?ContentScraper
resultados_h1 <- ContentScraper(
Url = "./lyceemermoz.edu.ar-231729/",  # Directorio donde están las páginas descargadas
CssPatterns = ".h1"  # Selecciona el patrón CSS de lo que quieres extraer
)
resultados_h1 <- ContentScraper(
Url = "./lyceemermoz.edu.ar-231729/",  # Directorio donde están las páginas descargadas
CssPatterns = "h1."  # Selecciona el patrón CSS de lo que quieres extraer
)
resultados_h1 <- ContentScraper(
Url = "./lyceemermoz.edu.ar-231729/",  # Directorio donde están las páginas descargadas
CssPatterns = "h2."  # Selecciona el patrón CSS de lo que quieres extraer
)
resultados_h1 <- ContentScraper(
Url = "./lyceemermoz.edu.ar-231729/",  # Directorio donde están las páginas descargadas
CssPatterns = c("h2.")  # Selecciona el patrón CSS de lo que quieres extraer
)
resultados_h1 <- ContentScraper(
Url = "./lyceemermoz.edu.ar-231729/",  # Directorio donde están las páginas descargadas
CssPatterns = c("h2")  # Selecciona el patrón CSS de lo que quieres extraer
)
resultados_h1 <- ContentScraper(
Url = "./lyceemermoz.edu.ar-231729/",  # Directorio donde están las páginas descargadas
CssPatterns = c("a")  # Selecciona el patrón CSS de lo que quieres extraer
)
resultados_h1 <- ContentScraper(
Url = "./lyceemermoz.edu.ar-231729/",  # Directorio donde están las páginas descargadas
CssPatterns = c(".elementor-heading-title elementor-size-default"), astext = T  # Selecciona el patrón CSS de lo que quieres extraer
)
resultados_h1 <- ContentScraper(
Url = "./lyceemermoz.edu.ar-231729/",  # Directorio donde están las páginas descargadas
CssPatterns = c(".elementor-heading-title elementor-size-default"), astext = T,
PatternsName = c("title","content"))
resultados_h1 <- ContentScraper(
Url = "./lyceemermoz.edu.ar-231729/",  # Directorio donde están las páginas descargadas
CssPatterns = c(".elementor-heading-title elementor-size-default"), astext = T,
PatternsName = c("title"))
Rcrawler(Website = "https://lyceemermoz.edu.ar/v5/pedagogie/",
no_cores = 4, # numero de procesos que ejecutará la tarea
no_conn = 4)
resultados_h1 <- ContentScraper(
Url = "./lyceemermoz.edu.ar-231729/",  # Directorio donde están las páginas descargadas
CssPatterns = c(".a"), astext = T,
PatternsName = c("href"))
enlaces <- ContentScraper(
Url = "./lyceemermoz.edu.ar-231729/",
CssPatterns = c("a"),  # Busca los elementos <a>
Attributes = c("href")  # Devuelve los atributos "href" de esos elementos
)
enlaces <- ContentScraper(
Url = "./lyceemermoz.edu.ar-231729/",
CssPatterns = c("a")  # Devuelve los atributos "href" de esos elementos
)
enlaces <- ContentScraper(
Url = "./lyceemermoz.edu.ar-231729/1.html",
CssPatterns = c("a")  # Devuelve los atributos "href" de esos elementos
)
enlaces <- ContentScraper(
Url = "./lyceemermoz.edu.ar-231729/1 .html",
CssPatterns = c("a")  # Devuelve los atributos "href" de esos elementos
)
enlaces <- ContentScraper(
Url = "./lyceemermoz.edu.ar-231729/1 .html",
CssPatterns = c(".a")  # Devuelve los atributos "href" de esos elementos
)
enlaces <- ContentScraper(
Url = "./lyceemermoz.edu.ar-231729/1 .html",
CssPatterns = c(".h2")  # Devuelve los atributos "href" de esos elementos
)
list.files("./lyceemermoz.edu.ar-231729/", recursive = TRUE)
enlaces <- ContentScraper(
Url = "./lyceemermoz.edu.ar-231729/1.html",  # Archivo HTML específico
CssPatterns = c("a"),  # Busca elementos <a>
Attributes = c("href")  # Extrae el atributo href
)
html <- read_html("./lyceemermoz.edu.ar-231729/1.html")
library(rvest)
html <- read_html("./lyceemermoz.edu.ar-231729/1.html")
# Extraer todos los enlaces
enlaces <- html %>% html_nodes("a") %>% html_attr("href")
enlaces
ListProjects()
Rcrawler( Website = "./lyceemermoz.edu.ar-231729/1.html",
ExtractXpathPat = "//*/a/@href",
ManyPerPattern = TRUE)`
View(INDEX)
View(INDEX)
Allinks<-unlist(lapply(DATA, `[[`, 2))
Rcrawler(Website = "./lyceemermoz.edu.ar-231729/1.html", no_cores = 4, no_conn = 4, ExtractXpathPat = c("//h1","//article"), PatternsNames = c("Title","Content"))
Rcrawler(Website = "./lyceemermoz.edu.ar-231729/1.html", no_cores = 4, no_conn = 4, ExtractXpathPat = c("//h1"))
Rcrawler(Website = "./lyceemermoz.edu.ar-231729/1.html", no_cores = 4, no_conn = 4, ExtractXpathPat = c("//h1"))
View(INDEX)
Rcrawler(Website = "./lyceemermoz.edu.ar-231729/1.html",
no_cores = 4, no_conn = 4, ExtractXpathPat = c("//h2"))
Rcrawler(Website = "https://www.diariodepontevedra.es/tags/buenos-aires/",
no_cores = 4, # numero de procesos que ejecutará la tarea
no_conn = 4)
list.files("./diariodepontevedra.es-271525/", recursive = TRUE)
resultados_h1 <- ContentScraper(
Url = "./diariodepontevedra.es-271525/",  # Directorio donde están las páginas descargadas
CssPatterns = c(".a"), astext = T,
PatternsName = c("href"))
links<-ContentScraper(Url = "./diariodepontevedra.es-271525/1 .html",
XpathPatterns = "//*/a/@href" ,
ManyPerPattern = TRUE)
links<-ContentScraper(Url = "./diariodepontevedra.es-271525/1.html",
XpathPatterns = "//*/a/@href" ,
ManyPerPattern = TRUE)
links<-ContentScraper(Url = "./diariodepontevedra.es-271525/2 .html",
XpathPatterns = "//*/a/@href" ,
ManyPerPattern = TRUE)
links<-ContentScraper(Url = "./diariodepontevedra.es-271525/2 .html",
XpathPatterns = "//*/a/href" ,
ManyPerPattern = TRUE)
links<-ContentScraper(Url = "./diariodepontevedra.es-271525/1.html",
XpathPatterns = "//*/a/@href" ,
ManyPerPattern = TRUE)
links<-ContentScraper(Url = "./diariodepontevedra.es-271525/3 .html",
XpathPatterns = "//*/a/@href" ,
ManyPerPattern = TRUE)
Rcrawler(Website = "./diariodepontevedra.es-271525/1 .html", no_cores = 4, no_conn = 4, ExtractCSSPat =
c(".entry-title",".entry-content"), PatternsNames = c("Title","Content"))
Rcrawler(Website = "./diariodepontevedra.es-271525/1 .html", no_cores = 4, no_conn = 4, ExtractCSSPat =
c(".h2",".entry-content"), PatternsNames = c("title","Content"))
Rcrawler(Website = "./diariodepontevedra.es-271525/1 .html", no_cores = 4, no_conn = 4, ExtractCSSPat =
c(".h2"), PatternsNames = c("title"))
Rcrawler(Website = "./diariodepontevedra.es-271525/1.html", no_cores = 4, no_conn = 4, ExtractCSSPat =
c(".h2"), PatternsNames = c("title"))
Rcrawler(Website = "./diariodepontevedra.es-271525/1.html", no_cores = 4, no_conn = 4, ExtractCSSPat =
c(".h2"))
links<-ContentScraper(Url = "./diariodepontevedra.es-271525/3 .html",
XpathPatterns = "//*/a/@href" ,
ManyPerPattern = TRUE)
Rcrawler(Website = "http://books.toscrape.com/catalogue/sapiens-a-brief-history-of-humankind_996/index.html",
no_cores = 4, # numero de procesos que ejecutará la tarea
no_conn = 4)
Rcrawler(Website = "http://books.toscrape.com/catalogue/",
no_cores = 4, # numero de procesos que ejecutará la tarea
no_conn = 4)
Rcrawler(Website = "http://books.toscrape.com/",
no_cores = 4, # numero de procesos que ejecutará la tarea
no_conn = 4)
Rcrawler(Website = "http://books.toscrape.com/",
no_cores = 4, # numero de procesos que ejecutará la tarea
no_conn = 4)
Rcrawler(Website = "https://books.toscrape.com/",
no_cores = 4, # numero de procesos que ejecutará la tarea
no_conn = 4)
Rcrawler(Website = "http://quotes.toscrape.com/",
no_cores = 4, # numero de procesos que ejecutará la tarea
no_conn = 4)
Rcrawler(Website = "./quotes.toscrape.com-271552/1 .html", no_cores = 4, no_conn = 4, ExtractCSSPat =
c(".h2"))
links<-ContentScraper(Url = "./quotes.toscrape.com-271552/1 .html",
XpathPatterns = "//*/a/@href" ,
ManyPerPattern = TRUE)
Rcrawler(Website = "./quotes.toscrape.com-271552/1 .html", no_cores = 4, no_conn = 4, ExtractCSSPat =
c(".div"))
links<-ContentScraper(Url = "./quotes.toscrape.com-271552/1 aa.html",
XpathPatterns = "//*/a/@href" ,
ManyPerPattern = TRUE)
links<-ContentScraper(Url = "./quotes.toscrape.com-271552/1 .html",
XpathPatterns = "//*/a/@href" ,
ManyPerPattern = TRUE)
list.files("./quotes.toscrape.com-271552/", recursive = TRUE)
ListProjects()
MyDATA<-LoadHTMLFiles("quotes.toscrape.com-271552", type = "vector")
library(rvest)
pagina <- read_html("./quotes.toscrape.com-271552/1 .html")
aber <- pagina %>%
html_elements("h1")
aber <- pagina %>%
html_elements("h1") %>%
html_text()
pagina <- read_html("./quotes.toscrape.com-271552/3 .html")
aber <- pagina %>%
html_elements("h3") %>%
html_text()
Rcrawler(Website = "./quotes.toscrape.com-271552/3 .html", no_cores = 4, no_conn = 4, ExtractCSSPat =
c(".h3"))
Rcrawler(Website = "./quotes.toscrape.com-271552/3 .html", no_cores = 4, no_conn = 4, ExtractCSSPat =
c(".author-title"))
Rcrawler(Website = "./quotes.toscrape.com-271552", no_cores = 4, no_conn = 4, ExtractCSSPat =
c(".author-title"))
c(".author-title",PatternsNames = c("container"))
c(".author-title",PatternsNames = c("container"))
ExtractCSSPat = c(".author-title",PatternsNames = c("container"))
ExtractCSSPat = c(".author-title",PatternsNames = c("container"))
Rcrawler(Website = "./quotes.toscrape.com-271552/3 .html", no_cores = 4, no_conn = 4,
ExtractCSSPat = c(".author-title"),PatternsNames = c("container"))
links<-ContentScraper(Url = "./quotes.toscrape.com-271552/3 .html",
XpathPatterns = "//*/a/@href" ,
ManyPerPattern = TRUE)
links<-ContentScraper(Url = "https://quotes.toscrape.com",
XpathPatterns = "//*/a/@href" ,
ManyPerPattern = TRUE)
View(links)
links[[1]]
Rcrawler(Website = "https://quotes.toscrape.com", no_cores = 4, no_conn = 4,
ExtractCSSPat = c(".author-title"), PatternsNames = c("title"))
Rcrawler(Website = "quotes.toscrape.com-271552", no_cores = 4, no_conn = 4,
ExtractCSSPat = c(".author-title"), PatternsNames = c("title"))
Rcrawler(Website = "./quotes.toscrape.com-271552", no_cores = 4, no_conn = 4,
ExtractCSSPat = c(".author-title"), PatternsNames = c("title"))
Rcrawler(Website = "./quotes.toscrape.com-271552/", no_cores = 4, no_conn = 4,
ExtractCSSPat = c(".author-title"), PatternsNames = c("title"))
Rcrawler(Website = "https://quotes.toscrape.com", no_cores = 4, no_conn = 4,
ExtractCSSPat = c(".author-description"), PatternsNames = c("Description"))
Rcrawler(Website = "https://quotes.toscrape.com", no_cores = 4, no_conn = 4,
ExtractCSSPat = c(".author-description"))
Rcrawler(Website = "./quotes.toscrape.com-271552", no_cores = 4, no_conn = 4,
ExtractCSSPat = c(".author-description"))
Rcrawler(Website = "https://quotes.toscrape.com", no_cores = 4, no_conn = 4,
ExtractCSSPat = c(".author-description"))
View(INDEX)
View(links)
links[[1]]
links<-ContentScraper(Url = "./quotes.toscrape.com-271552",
XpathPatterns = "//*/a/@href" ,
ManyPerPattern = TRUE)
links<-ContentScraper(Url = "./quotes.toscrape.com-271552/",
XpathPatterns = "//*/a/@href" ,
ManyPerPattern = TRUE)
links<-ContentScraper(Url = "MyDATA",
XpathPatterns = "//*/a/@href" ,
ManyPerPattern = TRUE)
links<-ContentScraper(Url = "https://quotes.toscrape.com",
XpathPatterns = "//*/a/@href" ,
ManyPerPattern = TRUE)
Rcrawler(Website = "MyDATA", no_cores = 4, no_conn = 4,
ExtractCSSPat = c(".author-title"), PatternsNames = c("title"))
links<-ContentScraper(Url = "https://quotes.toscrape.com",
XpathPatterns = "//*/a/@href" ,
ManyPerPattern = TRUE)
View(links)
links[[1]]
links[[1]]
unlist(links)
Rcrawler(Website = "https://quotes.toscrape.com",
no_cores = 4, # numero de procesos que ejecutará la tarea
no_conn = 4) # numero de conexiones simultaneas
ListProjects()
links<-ContentScraper(Url = "https://quotes.toscrape.com",
XpathPatterns = "//*/a/@href" ,
ManyPerPattern = TRUE)
unlist(links)
Rcrawler(Website = "https://quotes.toscrape.com", no_cores = 4, no_conn = 4,
ExtractCSSPat = c(".author-title"), PatternsNames = c("title"))
Rcrawler(Website = "https://quotes.toscrape.com", no_cores = 4, no_conn = 4,
ExtractCSSPat = c(".author-description"))
Rcrawler(Website = "https://quotes.toscrape.com", no_cores = 4, no_conn = 4,
ExtractCSSPat = c(".author-title",".author-description"), PatternsNames = c("title",
"description"))
knitr::opts_chunk$set(echo = TRUE)
rd <- rsDriver(browser = "firefox",
chromever = NULL,
port= 4455L)
library(RSelenium)
library(tidyverse)
library(rvest)
library(Rcrawler)
rd <- rsDriver(browser = "firefox",
chromever = NULL,
port= 4455L)
remDr <- rd[["client"]]
remDr$navigate("https://www.infobae.com/")
remDr$maxWindowSize()
remDr$findElements("id", "onesignal-slidedown-cancel-button")[[1]]$clickElement()
remDr$findElements("class", "menu-item")[[1]]$clickElement()
remDr$findElements("text", "España")[[1]]$clickElement()
remDr$findElements("href", "https://www.infobae.com/espana/")[[1]]$clickElement()
remDr$findElements("link text", "https://www.infobae.com/espana/")[[1]]$clickElement()
xpath_boton_espania <- "//a[@href='https://www.infobae.com/espana/']"
boton_espania <- remDr$findElement(using = "xpath", value = xpath_boton_espania)
boton_espania$getElementText
boton_espania$click()
xpath_boton_espania <- "//a[@href='https://www.infobae.com/espana/']"
boton_espania <- remDr$findElement(using = "xpath", value = xpath_boton_espania)
boton_espania$click()
remDr$navigate("https://www.infobae.com/")
remDr$maxWindowSize()
xpath_boton_espania <- "//a[@href='https://www.infobae.com/espana/']"
boton_espania <- remDr$findElement(using = "xpath", value = xpath_boton_espania)
boton_espania$click()
boton_espania$clickElement()
remDr$findElements("id", "hamburger-icon")[[1]]$clickElement()
remDr$findElements("id", "search-icon")[[1]]$clickElement()
search_bar <- remDr$findElement(using = "id", value = "queryly_query")
search_bar$sendKeysToElement(list("deportivo la coruña campeon"))
remDr$executeScript("window.scrollTo(0, document.body.scrollHeight);")
remDr$executeScript("window.scrollTo(0, document.body.scrollHeight);")
# Función para desplazarse al final de la página
scroll_to_bottom <- function(driver, max_attempts = 10, delay = 2) {
for (i in 1:max_attempts) {
# Obtener la altura actual de la página
previous_height <- driver$executeScript("return document.body.scrollHeight")
# Desplazarse al final
driver$executeScript("window.scrollTo(0, document.body.scrollHeight);")
# Esperar un momento para permitir que se cargue el contenido
Sys.sleep(delay)
# Verificar si la altura cambió
new_height <- driver$executeScript("return document.body.scrollHeight")
if (new_height == previous_height) {
message("Se alcanzó el final de la página.")
break
}
}
}
# Llamar a la función para desplazarse al final de la página
scroll_to_bottom(remDr)
new_height
# Obtener la altura actual de la página
previous_height <- driver$executeScript("return document.body.scrollHeight")
# Obtener la altura actual de la página
previous_height <- remDr$executeScript("return document.body.scrollHeight")
previous_height
# Desplazarse al final
driver$executeScript("window.scrollTo(0, document.body.scrollHeight);")
# Desplazarse al final
remDr$executeScript("window.scrollTo(0, document.body.scrollHeight);")
# Verificar si la altura cambió
new_height <- remDr$executeScript("return document.body.scrollHeight")
new_height
new_height == previous_height
new_height[1]
new_height[[1]]
new_height[[1]] == previous_height[[1]]
for (i in 1:max_attempts) {
# Obtener la altura actual de la página
previous_height <- remDr$executeScript("return document.body.scrollHeight")
# Desplazarse al final
remDr$executeScript("window.scrollTo(0, document.body.scrollHeight);")
# Esperar un momento para permitir que se cargue el contenido
Sys.sleep(delay)
# Verificar si la altura cambió
new_height <- remDr$executeScript("return document.body.scrollHeight")
if (new_height[[1]] == previous_height[[1]]) {
message("Se alcanzó el final de la página.")
break
}
}
# Función para desplazarse al final de la página
scroll_to_bottom <- function(driver, max_attempts = 10, delay = 2) {
for (i in 1:max_attempts) {
# Obtener la altura actual de la página
previous_height <- remDr$executeScript("return document.body.scrollHeight")
# Desplazarse al final
remDr$executeScript("window.scrollTo(0, document.body.scrollHeight);")
# Esperar un momento para permitir que se cargue el contenido
Sys.sleep(delay)
# Verificar si la altura cambió
new_height <- remDr$executeScript("return document.body.scrollHeight")
if (new_height[[1]] == previous_height[[1]]) {
message("Se alcanzó el final de la página.")
break
}
}
}
# Llamar a la función para desplazarse al final de la página
scroll_to_bottom(remDr)
remDr$findElements("id", "onesignal-slidedown-cancel-button")[[1]]$clickElement()
xpath_boton_espania <- "//a[@href='https://www.infobae.com/espana/']"
boton_espania <- remDr$findElement(using = "xpath", value = xpath_boton_espania)
boton_espania$clickElement()
remDr$findElements("id", "hamburger-icon")[[1]]$clickElement()
remDr$findElements("id", "search-icon")[[1]]$clickElement()
search_bar <- remDr$findElement(using = "id", value = "queryly_query")
search_bar$sendKeysToElement(list("deportivo la coruña campeon"))
# Función para desplazarse al final de la página
scroll_to_bottom <- function(driver, max_attempts = 10, delay = 2) {
for (i in 1:max_attempts) {
# Obtener la altura actual de la página
previous_height <- remDr$executeScript("return document.body.scrollHeight")
# Desplazarse al final
remDr$executeScript("window.scrollTo(0, document.body.scrollHeight);")
# Esperar un momento para permitir que se cargue el contenido
Sys.sleep(delay)
# Verificar si la altura cambió
new_height <- remDr$executeScript("return document.body.scrollHeight")
if (new_height[[1]] == previous_height[[1]]) {
message("Se alcanzó el final de la página.")
break
}
}
}
# Llamar a la función para desplazarse al final de la página
scroll_to_bottom(remDr)
page_source <- remDr$getPageSource()[[1]]
page <- read_html(page_source)
raiz <- "https://www.infobae.com"
links <- links[!duplicated(links) ]
raiz <- "https://www.infobae.com"
links_completos <- str_c(raiz, links)
links_completos
links <- page %>%
html_nodes("div.queryly_item_title") %>%
html_elements("a") %>%
html_attr("href")
links
links <- links[!duplicated(links) ]
raiz <- "https://www.infobae.com"
links_completos <- str_c(raiz, links)
links_completos
tabla_noticias <- function(links){
noticia <- read_html(links)
titulo <- noticia %>%
html_element("h1") %>%
html_text() %>%
str_squish()
cuerpo <- noticia %>%
html_element("p.paragraph") %>%
html_text() %>%
str_squish()
fecha  <- noticia %>%
html_element("span.sharebar-article-date") %>%
html_text() %>%
str_squish()
autor  <- noticia %>%
html_element("span.author-name") %>%
html_text() %>%
str_squish()
web <- links
tabla_final <- as.data.frame(cbind("titulo"=titulo, "autor" = autor,  "cuerpo"=cuerpo,
"fecha" = fecha, "Link" = web
))
return(tabla_final)
}
noticias <- tibble(
titulo=character(),
cuerpo=character(),
fecha=character(),
autor= character()
)
print(paste("Se encontraron", length(links_completos), "noticias\n"))
for(i in 1:length(links_completos)){
print(paste("Obteniendo información de noticia:\n ",links_completos[i]))
resultado <- tabla_noticias(links_completos[i])
noticias <- noticias %>%
bind_rows(resultado)
}
View(noticias)
knitr::opts_chunk$set(echo = TRUE)
rd <- rsDriver(browser = "firefox",
chromever = NULL,
port= 4453L)
library(RSelenium)
library(tidyverse)
library(rvest)
library(Rcrawler)
rd <- rsDriver(browser = "firefox",
chromever = NULL,
port= 4453L)
remDr <- rd[["client"]]
remDr$navigate("https://www.infobae.com/")
remDr$maxWindowSize()
remDr$findElements("id", "onesignal-slidedown-cancel-button")[[1]]$clickElement()
remDr$findElements("id", "hamburger-icon")[[1]]$clickElement()
remDr$findElements("id", "search-icon")[[1]]$clickElement()
Sys.sleep(5)
search_bar <- remDr$findElement(using = "id", value = "queryly_query")
search_bar$sendKeysToElement(list("web scraping"))
close()
close.connection()
rd <- rsDriver(browser = "firefox",
chromever = NULL,
port= 4454L)
remDr <- rd[["client"]]
remDr$navigate("https://www.infobae.com/")
remDr$maxWindowSize()
Sys.sleep(5)
remDr$findElements("id", "onesignal-slidedown-cancel-button")[[1]]$clickElement()
Sys.sleep(3)
remDr$findElements("id", "hamburger-icon")[[1]]$clickElement()
Sys.sleep(3)
remDr$findElements("id", "search-icon")[[1]]$clickElement()
search_bar <- remDr$findElement(using = "id", value = "queryly_query")
search_bar$sendKeysToElement(list("web scraping"))
pagina_fuente <- remDr$getPageSource()[[1]]
pagina <- read_html(pagina_fuente)
titles <- page %>% html_nodes("div.queryly_item_title") %>%
html_text() %>%
str_squish()
titles <- pagina %>% html_nodes("div.queryly_item_title") %>%
html_text() %>%
str_squish()
length(titles)
