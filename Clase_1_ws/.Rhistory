library(tidyverse)
library(rvest)
library(stringr)
library(readr)
library(tidyverse)
html_discurso <- read_html("https://www.lamoncloa.gob.es/presidente/intervenciones/Paginas/2023/prsp15112023.aspx" , encoding = "UTF-8")
View(html_discurso)
xml_child(html_discurso, 1)
xml_child(html_discurso, 2)
print(html_discurso)
html_discurso <- read_html("https://www.lamoncloa.gob.es/presidente/intervenciones/Paginas/2023/prsp15112023.aspx" , encoding = "UTF-8")
print(html_discurso)
discurso <- html_discurso %>%
html_element("p.Justificado")
discurso <- html_discurso %>%
html_element("p.Justificado")
print(discurso)
texto_plano <- discurso %>%
html_text2()
print(texto_plano)
?html_text2
discurso <- html_discurso %>%
html_elements("p.Justificado") %>%
html_text2()
print(discurso)
write_lines(discurso, "./discursos/discurso_ps.txt")
pagina_mx <- read_html("https://www.gob.mx/presidencia/articulos/version-estenografica-toma-de-protesta-de-claudia-sheinbaum-como-presidenta-constitucional-de-los-estados-unidos-mexicanos", encoding = "UTF-8")
head(pagina_mx)
print(pagina_mx)
extraer_texto <- pagina_mx %>%
html_elements("div.article-body") %>%
html_text()
print(extraer_texto)
extraer_texto <- str_replace_all(extraer_texto, pattern = "\r\n", replacement = "\n")
extraer_texto <- str_replace_all(extraer_texto, pattern = "\n\n", replacement = "\n")
print(extraer_texto)
pagina_mx <- read_html("https://www.gob.mx/presidencia/articulos/version-estenografica-toma-de-protesta-de-claudia-sheinbaum-como-presidenta-constitucional-de-los-estados-unidos-mexicanos", encoding = "UTF-8")
print(pagina_mx)
extraer_texto <- pagina_mx %>%
html_elements("div.article-body") %>%
html_text()
print(extraer_texto)
extraer_texto <- extraer_texto %>%
str_squish()
extraer_texto
ab <- pagina_mx %>%
html_elements("h1")
ab <- pagina_mx %>%
html_elements("h1") %>%
html_text()
link <- "https://www.anagrama-ed.es/libro/compendium/cuentos/9788433924216/CP_7"
resultado <- tabla_libro(link)
pagina_entera <- read_html("https://www.gob.mx/presidencia/articulos/version-estenografica-toma-de-protesta-de-claudia-sheinbaum-como-presidenta-constitucional-de-los-estados-unidos-mexicanos")
elemento_titulo <- pagina_entera %>%
html_element("h1.bottom-buffer")
print(elemento_titulo)
elemento_titulo <- pagina_entera %>%
html_element("h1")
print(elemento_titulo)
elemento_titulo <- pagina_entera %>%
html_element("h1") %>%
html_text()
print(elemento_titulo)
print(elemento_titulo)
elemento_subtitulo <- pagina_entera %>%
html_element("h2") %>%
html_text()
print(elemento_subtitulo)
descarga <- read_html("https://www.anagrama-ed.es/libro/panorama-de-narrativas/albertine-desaparecida/9788433931320/PN_132")
titulo <- descarga %>%
html_element("h1.titulo-libro") %>%
html_text2()
print(titulo)
biografia <- descarga %>%
html_element("div.textContent") %>%
html_text2()
print(biografia)
biografia_2 <- descarga %>%
html_elements("p") %>%
html_text()
print(biografia_2)
biografia_2 <- descarga %>%
html_elements("p") %>%
html_text2()
print(biografia_2)
biografia_2 <- descarga %>%
html_elements("p") %>%
html_text()
print(biografia_2)
biografia_2 <- descarga %>%
html_elements("p") %>%
html_text2()
print(biografia_2)
biografia <- descarga %>%
html_element("div.textContent") %>%
html_text2()
print(biografia)
biografia <- descarga %>%
html_element("#col_right") %>%
html_element("div.textContent") %>%
html_text2()
print(biografia)
autor <- descarga %>%
html_element("#col_right") %>%
html_element("p.t24px") %>%
html_text2()
autor
biografia
biografia_2
autor <- descarga %>%
html_element("#col_right") %>%
html_element("p.t24px") %>%
html_text2()
autor
autor <- descarga %>%
html_element("#col_right") %>%
html_element("p") %>%
html_text2()
autor
tabla_info <- descarga %>%
html_element("table.no-print") %>%
html_table()
tabla_info
View(tabla_info)
tabla_info_2 <- descarga %>%
html_element("div.tab-content") %>%
html_table()
View(tabla_info_2)
tabla_info_2 <- descarga %>%
html_element("#tab-content-info") %>%
html_table()
View(tabla_info_2)
View(tabla_info_2)
tabla_info%>%
pivot_wider(names_from = "X1", values_from = "X2")
tabla_info%>%
pivot_wider(names_from = "X1", values_from = "X2") %>%
janitor::clean_names()
tabla_info2 <- tabla_info%>%
pivot_wider(names_from = "X1", values_from = "X2") %>%
janitor::clean_names() %>%
rename("precio" = "pvp_con_iva",
"paginas" = "num_de_paginas",
"fecha_publicacion" = "publicacion") %>%
dplyr::select(-codigo)
print(tabla_info2)
View(tabla_info2)
tabla_final <- as.data.frame(cbind("Autor" = autor,"Biografia"=biografia,
tabla_info2))
View(tabla_final)
tabla_libro <- function(link){
descarga <- read_html(link)
titulo <- descarga %>%
html_element("h1.titulo-libro") %>%
html_text2()
biografia <- descarga %>%
html_element("#col_right") %>%
html_element("div.textContent") %>%
html_text2()
autor <- descarga %>%
html_element("#col_right") %>%
html_element("p.t24px") %>%
html_text2()
tabla_info <- descarga %>%
html_element("table.no-print") %>%
html_table()
tabla_info2 <- tabla_info%>%
pivot_wider(names_from = "X1", values_from = "X2") %>%
janitor::clean_names() %>%
rename("precio" = "pvp_con_iva",
"paginas" = "num_de_paginas",
"fecha_publicacion" = "publicacion") %>%
dplyr::select(-codigo)
tabla_final <- as.data.frame(cbind("Autor" = autor,"Biografia"=biografia,
tabla_info2))
return(tabla_final)
}
link <- "https://www.anagrama-ed.es/libro/panorama-de-narrativas/tierra-de-empusas/9788433929716/PN_1142"
libro_nuevo <- tabla_libro(link = link)
View(libro_nuevo)
pagina <- read_html("https://www.anagrama-ed.es/autor/almodovar-pedro-38")
libros <- pagina %>%
html_elements("div.libro-vertical__portada") %>%
html_elements("a") %>%
html_attr("href")
libros <- pagina %>%
html_elements("div.libro-vertical__portada") %>%
html_elements("a") %>%
html_attr("href")
print(libros)
raiz <- "https://www.anagrama-ed.es"
libros_link <- str_c(raiz, libros)
libros_link
for (i in libros_link){
print(i)
}
tabla_libro(libros_link[1])
books <- tibble(
Autor=character(),
Biografia=character(),
isbn=character(),
ean=character(),
precio= character(),
paginas=character(),
coleccion=character(),
fecha_publicacion = character(),
otras_ediciones = character()
)
View(books)
libros_link
for(i in libros_link){
resultado <- tabla_libro(i)
books <- books %>%
bind_rows(resultado)
}
View(books)
as.Date("2025-01-20")+days(60)
library(tidyverse)
as.Date("2025-01-20")+days(60)
library(tidyverse)
library(tidymodels)
# Cargar el dataset
data <- read.csv("https://raw.githubusercontent.com/data-datum/datasets_ml/refs/heads/main/AirQualityUCI.csv", sep = ";", dec = ",", na.strings = c("NA", "-200"))
# Verificar nombres de columnas
names(data)
# Seleccionar las variables de interés y eliminar valores faltantes
data <- data %>%
select(PT08.S5.O3., T, RH, NOx.GT., NO2.GT., NMHC.GT.) %>%
drop_na()
set.seed(123)  # Para reproducibilidad
split <- initial_split(data, prop = 0.8, strata = PT08.S5.O3.)
train_data <- training(split)
test_data <- testing(split)
train_data
test_data
# Configurar la validación cruzada
set.seed(123)
folds <- vfold_cv(train_data, v = 5, strata = PT08.S5.O3.)
folds$splits
#para ver que muestra cae en cada fold
map(folds$splits, "in_id")
# Crear la receta de preprocesamiento
rf_recipe <- recipe(PT08.S5.O3. ~ T + RH + NOx.GT. + NO2.GT. + NMHC.GT., data = train_data) %>%
step_normalize(all_predictors())  # Estandarizar las variables predictoras
# Definir el modelo de árbol de decisión
tree_model <- decision_tree(
cost_complexity = tune(),      # Hiperparámetro para ajustar
tree_depth = tune(),            # Hiperparámetro para ajustar
min_n = tune()                  # Hiperparámetro para ajustar
) %>%
set_engine("rpart") %>%
set_mode("regression")
# Crear un workflow que combine la receta y el modelo
workflow_tree <- workflow() %>%
add_recipe(rf_recipe) %>%
add_model(tree_model)
# Definir una cuadrícula para los hiperparámetros a explorar
tree_grid <- grid_regular(
cost_complexity(),
tree_depth(),
min_n(),
levels = 5  # Ajusta el número de niveles según tus necesidades
)
tree_grid
doParallel::registerDoParallel() #paralelizamos los cálculos
set.seed(345)
# Realizar el ajuste de hiperparámetros
tune_results <- tune_grid(
workflow_tree,
resamples = folds,
grid = tree_grid,
metrics = metric_set(rmse, rsq, mae)
)
tune_results
# Ver los resultados
collect_metrics(tune_results)
# Seleccionar el mejor conjunto de hiperparámetros
best_params <- select_best(tune_results, metric="mae")
# Finalizar el modelo con los mejores hiperparámetros
final_workflow <- finalize_workflow(workflow_tree, best_params)
# Entrenar el modelo final en el conjunto de entrenamiento completo
final_model <- last_fit(final_workflow, split, metrics=metric_set(rmse, mae))
final_model
final_model %>%
collect_metrics()
final_model %>%
collect_predictions()
# Obtener los valores verdaderos y predichos
predictions <- final_model %>%
collect_predictions()
# Crear el gráfico de valores reales vs. valores predichos
ggplot(predictions, aes(x = PT08.S5.O3., y = .pred)) +
geom_point(color = "blue", alpha = 0.6) +  # Puntos azules para las observaciones
geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") +  # Línea de igualdad
labs(
title = "Predicted vs Real Values of PT08.S5(O3)",
x = "Real Values (PT08.S5(O3))",
y = "Predicted Values"
) +
theme_minimal()
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(lubridate) # Librería para trabajar con fechas
library(RSocrata) # Librería para acceder a datos desde Socrata Open Data API
library(tidymodels)
# Calculamos la fecha exacta de hace 2 años desde el día de hoy
years_ago <- today() - years(2)
# Construimos la URL para acceder a los datos de accidentes de tráfico en Chicago
# Usamos glue para insertar la fecha calculada como filtro en la URL y obtener solo datos de los últimos dos años
crash_url <- glue::glue("https://data.cityofchicago.org/Transportation/Traffic-Crashes-Crashes/85ca-t3if?$where=CRASH_DATE > '{years_ago}'")
# Leemos los datos desde la URL usando read.socrata y los convertimos en un tibble para facilitar su manipulación en R
crash_raw <- as_tibble(read.socrata(crash_url))
crash <- crash_raw %>%
arrange(desc(crash_date)) %>% # Ordenamos los registros en orden descendente
transmute( # Seleccionamos y transformamos variables con 'transmute'
injuries = if_else(injuries_total > 0, "injuries", "none"), # Creamos v 'injuries' q indica si hubo  lesiones
crash_date,     # Conservamos la fecha y hora del accidente
crash_hour,
report_type = if_else(report_type == "", "UNKNOWN", report_type), # Establecemos "UNKNOWN" si 'report_type' está vacío
num_units,  # Seleccionamos varias variables relevantes para el análisis
posted_speed_limit,
weather_condition,
lighting_condition,
roadway_surface_cond,
first_crash_type,
trafficway_type,
prim_contributory_cause,
latitude, longitude
) %>%
na.omit() # Eliminamos las filas con valores NA
crash
set.seed(123)
crash_split <- initial_split(crash, strata = injuries, prop=0.75)
crash_train <- training(crash_split)
crash_test <- testing(crash_split)
library(themis)
# Creamos una receta para el modelo con la variable de respuesta 'injuries' y el resto de variables predictoras en 'crash_train'
crash_rec <- recipe(injuries ~ ., data = crash_train) %>%
# Convertimos 'injuries' a un factor
step_mutate(injuries = as.factor(injuries)) %>%
# Extraemos componentes de fecha a partir de 'crash_date' para convertirla en variables como año, mes, día, etc.
step_date(crash_date) %>%
# Eliminamos la columna original 'crash_date' después de extraer sus componentes de fecha
step_rm(crash_date) %>%
# Agrupamos niveles menos frecuentes de variables categóricas bajo una nueva categoría llamada "OTHER"
step_other(
weather_condition,          # Condición del clima en el momento del accidente
first_crash_type,           # Tipo inicial del accidente
trafficway_type,            # Tipo de vía donde ocurrió el accidente
prim_contributory_cause,    # Causa primaria que contribuyó al accidente
other = "OTHER"             # Nombre de la categoría "otros" para agrupar niveles raros
) %>%
# Realizamos downsampling en la variable de respuesta 'injuries' para equilibrar las clases en caso de desbalance
step_downsample(injuries)
crash_rec
tree_spec <- decision_tree() %>%
set_engine("rpart") %>%
set_mode("classification")
crash_wf <- workflow() %>%
add_recipe(crash_rec) %>%
add_model(tree_spec)
crash_wf
crash_train$injuries <- as.factor(crash_train$injuries)
crash_test$injuries <- as.factor(crash_test$injuries)
set.seed(123)
tree_fit <- tree_spec %>%
fit(injuries ~ ., data = crash_train)
#imprimo el modelo
tree_fit
results <- crash_train %>%
bind_cols(predict(tree_fit, crash_train) %>%
rename(.pred_tree = .pred_class))
#veamos nuestra nueva tabla
head(results)
metrics(results, truth = injuries, estimate = .pred_tree)
# Genera predicciones con probabilidades y clases
results_test <- predict(tree_fit, new_data = crash_test, type = "prob") %>%
bind_cols(predict(tree_fit, new_data = crash_test, type = "class") %>%
rename(.pred_tree = .pred_class)) %>%
bind_cols(crash_test)
head(results_test)
# Definimos las métricas que queremos: accuracy y AUC
selected_metrics <- metric_set(accuracy, sensitivity, specificity, f_meas)
selected_metrics(results_test, truth = injuries, estimate = .pred_tree)
# Calcular la matriz de confusión
library(yardstick)
conf_matrix <- conf_mat(data = results_test,
truth = injuries,
estimate = .pred_tree)
# Mostrar la matriz de confusión
print(conf_matrix)
crash_fit <- last_fit(crash_wf, crash_split)
collect_predictions(crash_fit) %>%
roc_curve(injuries, .pred_injuries) %>%
ggplot(aes(x = 1 - specificity, y = sensitivity)) +
geom_line(size = 1.5, color = "midnightblue") +
geom_abline(lty = 2, alpha = 0.5, color = "gray50", size = 1.2) +
coord_equal()
collect_predictions(crash_fit) %>%
roc_curve(injuries, .pred_injuries) %>%
ggplot(aes(x = 1 - specificity, y = sensitivity)) +
geom_line(size = 1.5, color = "midnightblue") +
geom_abline(lty = 2, alpha = 0.5, color = "gray50", size = 1.2) +
coord_equal()
library(tidyverse)
library(lubridate) # Librería para trabajar con fechas
library(RSocrata) # Librería para acceder a datos desde Socrata Open Data API
library(tidymodels)
# Calculamos la fecha exacta de hace 2 años desde el día de hoy
years_ago <- today() - years(2)
# Construimos la URL para acceder a los datos de accidentes de tráfico en Chicago
# Usamos glue para insertar la fecha calculada como filtro en la URL y obtener solo datos de los últimos dos años
crash_url <- glue::glue("https://data.cityofchicago.org/Transportation/Traffic-Crashes-Crashes/85ca-t3if?$where=CRASH_DATE > '{years_ago}'")
# Leemos los datos desde la URL usando read.socrata y los convertimos en un tibble para facilitar su manipulación en R
crash_raw <- as_tibble(read.socrata(crash_url))
crash <- crash_raw %>%
arrange(desc(crash_date)) %>% # Ordenamos los registros en orden descendente
transmute( # Seleccionamos y transformamos variables con 'transmute'
injuries = if_else(injuries_total > 0, "injuries", "none"), # Creamos v 'injuries' q indica si hubo  lesiones
crash_date,     # Conservamos la fecha y hora del accidente
crash_hour,
report_type = if_else(report_type == "", "UNKNOWN", report_type), # Establecemos "UNKNOWN" si 'report_type' está vacío
num_units,  # Seleccionamos varias variables relevantes para el análisis
posted_speed_limit,
weather_condition,
lighting_condition,
roadway_surface_cond,
first_crash_type,
trafficway_type,
prim_contributory_cause,
latitude, longitude
) %>%
na.omit() # Eliminamos las filas con valores NA
crash
View(crash_raw)
View(crash)
set.seed(123)
crash_split <- initial_split(crash, strata = injuries, prop=0.75)
crash_train <- training(crash_split)
crash_test <- testing(crash_split)
library(themis)
# Creamos una receta para el modelo con la variable de respuesta 'injuries' y el resto de variables predictoras en 'crash_train'
crash_rec <- recipe(injuries ~ ., data = crash_train) %>%
# Convertimos 'injuries' a un factor
step_mutate(injuries = as.factor(injuries)) %>%
# Extraemos componentes de fecha a partir de 'crash_date' para convertirla en variables como año, mes, día, etc.
step_date(crash_date) %>%
# Eliminamos la columna original 'crash_date' después de extraer sus componentes de fecha
step_rm(crash_date) %>%
# Agrupamos niveles menos frecuentes de variables categóricas bajo una nueva categoría llamada "OTHER"
step_other(
weather_condition,          # Condición del clima en el momento del accidente
first_crash_type,           # Tipo inicial del accidente
trafficway_type,            # Tipo de vía donde ocurrió el accidente
prim_contributory_cause,    # Causa primaria que contribuyó al accidente
other = "OTHER"             # Nombre de la categoría "otros" para agrupar niveles raros
) %>%
# Realizamos downsampling en la variable de respuesta 'injuries' para equilibrar las clases en caso de desbalance
step_downsample(injuries)
View(crash_train)
View(crash_rec)
tree_spec <- decision_tree() %>%
set_engine("rpart") %>%
set_mode("classification")
crash_wf <- workflow() %>%
add_recipe(crash_rec) %>%
add_model(tree_spec)
crash_wf
crash_train$injuries <- as.factor(crash_train$injuries)
crash_test$injuries <- as.factor(crash_test$injuries)
set.seed(123)
tree_fit <- tree_spec %>%
fit(injuries ~ ., data = crash_train)
#imprimo el modelo
tree_fit
results <- crash_train %>%
bind_cols(predict(tree_fit, crash_train) %>%
rename(.pred_tree = .pred_class))
#veamos nuestra nueva tabla
head(results)
metrics(results, truth = injuries, estimate = .pred_tree)
# Genera predicciones con probabilidades y clases
results_test <- predict(tree_fit, new_data = crash_test, type = "prob") %>%
bind_cols(predict(tree_fit, new_data = crash_test, type = "class") %>%
rename(.pred_tree = .pred_class)) %>%
bind_cols(crash_test)
View(results_test)
head(results_test)
# Definimos las métricas que queremos: accuracy y AUC
selected_metrics <- metric_set(accuracy, sensitivity, specificity, f_meas)
selected_metrics(results_test, truth = injuries, estimate = .pred_tree)
# Calcular la matriz de confusión
library(yardstick)
conf_matrix <- conf_mat(data = results_test,
truth = injuries,
estimate = .pred_tree)
# Mostrar la matriz de confusión
print(conf_matrix)
crash_fit <- last_fit(crash_wf, crash_split)
collect_predictions(crash_fit) %>%
roc_curve(injuries, .pred_injuries) %>%
ggplot(aes(x = 1 - specificity, y = sensitivity)) +
geom_line(size = 1.5, color = "midnightblue") +
geom_abline(lty = 2, alpha = 0.5, color = "gray50", size = 1.2) +
coord_equal()
collect_predictions(crash_fit) %>%
roc_curve(injuries, .pred_injuries) %>%
ggplot(aes(x = 1 - specificity, y = sensitivity)) +
geom_line(size = 1.5, color = "midnightblue") +
geom_abline(lty = 2, alpha = 0.5, color = "gray50", size = 1.2) +
coord_equal()
as.Date("2025-01-20")+days(60)
