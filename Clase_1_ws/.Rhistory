mutate(txt = str_replace_all(txt, "[[:digit:]]", '')) %>%
select(-1)
stop_words <- read.csv("data/stop_words.csv")
unigram <- tabla_2 %>%
unnest_tokens(output = word, input= txt, token = "ngrams", n=1) %>%
na.omit() %>%
anti_join(stop_words)
nube_f <- unigram %>%
count(word, sort =TRUE)
return(nube_f)
}
nube <-  nube(noticias)
wordcloud2(nube)
grafico <- function(tabla){
ggplot(tabla[1:20,], aes(x=reorder(word,n), y=n))+
geom_col(aes(fill=word),col="black")+
coord_flip()+
labs(title = paste("Cantidad de menciones en notas"),
x="Palabra", y= "Frecuencia",
caption="Elaboración propia en base a web scraping del sitio 'Tiempo Argentino'")+
theme_minimal()+
theme(legend.position = "none")
}
grafico(nube)
nube <- function(tabla){
tabla_2 <- tabla %>%
select(cuerpo)
tabla_2 <- tabla_2 %>%
mutate(txt = str_to_lower(cuerpo))
tabla_2 <- tabla_2 %>%
mutate(txt = str_replace_all(txt, "[[:digit:]]", '')) %>%
select(-1)
stop_words <- read.csv("data/stop_words.csv")
unigram <- tabla_2 %>%
unnest_tokens(output = word, input= txt, token = "ngrams", n=1) %>%
na.omit() %>%
anti_join(stop_words)
nube_f <- unigram %>%
count(word, sort =TRUE)
return(nube_f)
}
nube <-  nube(noticias)
wordcloud2(nube)
grafico <- function(tabla){
ggplot(tabla[1:20,], aes(x=reorder(word,n), y=n))+
geom_col(aes(fill=word),col="black")+
coord_flip()+
labs(title = paste("Cantidad de menciones en notas"),
x="Palabra", y= "Frecuencia",
caption="Elaboración propia en base a web scraping del sitio 'Tiempo Argentino'")+
theme_minimal()+
theme(legend.position = "none")
}
grafico(nube)
View(noticias)
noticias$cuerpo[1]
nube <- function(tabla){
tabla_2 <- tabla %>%
select(cuerpo)
tabla_2 <- tabla_2 %>%
mutate(txt = str_to_lower(cuerpo))
tabla_2 <- gsub("\\.\\(function\\(.*?\\)\\);", "", tabla_2)
tabla_2 <- tabla_2 %>%
mutate(txt = str_replace_all(txt, "[[:digit:]]", '')) %>%
select(-1)
stop_words <- read.csv("data/stop_words.csv")
unigram <- tabla_2 %>%
unnest_tokens(output = word, input= txt, token = "ngrams", n=1) %>%
na.omit() %>%
anti_join(stop_words)
nube_f <- unigram %>%
count(word, sort =TRUE)
return(nube_f)
}
nube <-  nube(noticias)
nube <- function(tabla){
tabla_2 <- tabla %>%
select(cuerpo)
tabla_2 <- tabla_2 %>%
mutate(txt = str_to_lower(cuerpo))
tabla_2$cuerpo <- gsub("\\.\\(function\\(.*?\\)\\);", "", tabla_2$cuerpo)
tabla_2 <- tabla_2 %>%
mutate(txt = str_replace_all(txt, "[[:digit:]]", '')) %>%
select(-1)
stop_words <- read.csv("data/stop_words.csv")
unigram <- tabla_2 %>%
unnest_tokens(output = word, input= txt, token = "ngrams", n=1) %>%
na.omit() %>%
anti_join(stop_words)
nube_f <- unigram %>%
count(word, sort =TRUE)
return(nube_f)
}
nube <-  nube(noticias)
wordcloud2(nube)
grafico <- function(tabla){
ggplot(tabla[1:20,], aes(x=reorder(word,n), y=n))+
geom_col(aes(fill=word),col="black")+
coord_flip()+
labs(title = paste("Cantidad de menciones en notas"),
x="Palabra", y= "Frecuencia",
caption="Elaboración propia en base a web scraping del sitio 'Tiempo Argentino'")+
theme_minimal()+
theme(legend.position = "none")
}
grafico(nube)
print(noticias$cuerpo)
webs <- rx() %>%
rx_find('http')%>%
rx_anything_but(value = ' ')
nube <- function(tabla){
tabla_2 <- tabla %>%
select(cuerpo)
tabla_2 <- tabla_2 %>%
mutate(txt = str_to_lower(cuerpo))
tabla_2$cuerpo <- gsub("\\.\\(function\\(.*?\\)\\);", "", tabla_2$cuerpo)
webs <- rx() %>%
rx_find('http')%>%
rx_anything_but(value = ' ')
tabla_2 <- tabla_2 %>%
str_replace_all(., pattern = webs, "")
tabla_2 <- tabla_2 %>%
mutate(txt = str_replace_all(txt, "[[:digit:]]", '')) %>%
select(-1)
stop_words <- read.csv("data/stop_words.csv")
unigram <- tabla_2 %>%
unnest_tokens(output = word, input= txt, token = "ngrams", n=1) %>%
na.omit() %>%
anti_join(stop_words)
nube_f <- unigram %>%
count(word, sort =TRUE)
return(nube_f)
}
nube <-  nube(noticias)
webs <- rx() %>%
rx_find('http')%>%
rx_anything_but(value = ' ')
webs
nube <- function(tabla){
tabla_2 <- tabla %>%
select(cuerpo)
tabla_2 <- tabla_2 %>%
mutate(txt = str_to_lower(cuerpo))
tabla_2$cuerpo <- gsub("\\.\\(function\\(.*?\\)\\);", "", tabla_2$cuerpo)
webs <- rx() %>%
rx_find('http')%>%
rx_anything_but(value = ' ')
tabla_2$cuerpo <- gsub(web, "", tabla_2$cuerpo)
tabla_2 <- tabla_2 %>%
mutate(txt = str_replace_all(txt, "[[:digit:]]", '')) %>%
select(-1)
stop_words <- read.csv("data/stop_words.csv")
unigram <- tabla_2 %>%
unnest_tokens(output = word, input= txt, token = "ngrams", n=1) %>%
na.omit() %>%
anti_join(stop_words)
nube_f <- unigram %>%
count(word, sort =TRUE)
return(nube_f)
}
nube <-  nube(noticias)
nube <- function(tabla){
tabla_2 <- tabla %>%
select(cuerpo)
tabla_2 <- tabla_2 %>%
mutate(txt = str_to_lower(cuerpo))
tabla_2$cuerpo <- gsub("\\.\\(function\\(.*?\\)\\);", "", tabla_2$cuerpo)
webs <- rx() %>%
rx_find('http')%>%
rx_anything_but(value = ' ')
tabla_2$cuerpo <- gsub(webs, "", tabla_2$cuerpo)
tabla_2 <- tabla_2 %>%
mutate(txt = str_replace_all(txt, "[[:digit:]]", '')) %>%
select(-1)
stop_words <- read.csv("data/stop_words.csv")
unigram <- tabla_2 %>%
unnest_tokens(output = word, input= txt, token = "ngrams", n=1) %>%
na.omit() %>%
anti_join(stop_words)
nube_f <- unigram %>%
count(word, sort =TRUE)
return(nube_f)
}
nube <-  nube(noticias)
wordcloud2(nube)
grafico <- function(tabla){
ggplot(tabla[1:20,], aes(x=reorder(word,n), y=n))+
geom_col(aes(fill=word),col="black")+
coord_flip()+
labs(title = paste("Cantidad de menciones en notas"),
x="Palabra", y= "Frecuencia",
caption="Elaboración propia en base a web scraping del sitio 'Tiempo Argentino'")+
theme_minimal()+
theme(legend.position = "none")
}
grafico(nube)
nube <- function(tabla){
tabla_2 <- tabla %>%
select(cuerpo)
tabla_2 <- tabla_2 %>%
mutate(txt = str_to_lower(cuerpo))
tabla_2$cuerpo <- gsub("\\.\\(function\\(.*?\\)\\);", "", tabla_2$cuerpo)
webs <- rx() %>%
rx_find('http')%>%
rx_anything_but(value = ' ')
tabla_2$cuerpo <- gsub(webs, "", tabla_2$cuerpo)
tabla_2 <- tabla_2 %>%
mutate(txt = str_replace_all(txt, "[[:digit:]]", '')) %>%
select(-1)
stop_words <- read.csv("data/stop_words.csv")
unigram <- tabla_2 %>%
unnest_tokens(output = word, input= txt, token = "ngrams", n=1) %>%
na.omit() %>%
anti_join(stop_words)
nube_f <- unigram %>%
count(word, sort =TRUE)
return(nube_f)
}
nube <-  nube(noticias)
wordcloud2(nube)
grafico <- function(tabla){
ggplot(tabla[1:20,], aes(x=reorder(word,n), y=n))+
geom_col(aes(fill=word),col="black")+
coord_flip()+
labs(title = paste("Cantidad de menciones en notas"),
x="Palabra", y= "Frecuencia",
caption="Elaboración propia en base a web scraping del sitio 'Tiempo Argentino'")+
theme_minimal()+
theme(legend.position = "none")
}
grafico(nube)
?sf_use_s2
require(RSelenium)
require(rvest)
require(tidygeocoder)
require(dplyr)
require(sf)
require(RSelenium)
require(rvest)
require(tidygeocoder)
require(dplyr)
require(sf)
require(ggplot2)
require(ggimage)
require(leaflet)
require(leaflet.extras)
servidor <- rsDriver(browser = "firefox", port = 2312L, chromever = "108.0.5359.22")
rd <- rsDriver(browser = "firefox",
chromever = NULL,
port= 4461L)
remDr <- rd[["client"]]
palabra <- readline("¿Qué palabra clave estás buscando? ")
remDr$navigate(paste0("https://www.infobae.com/america/buscador/?query=", palabra))
remDr$findElements("id", "buscador_home_zocalo_1x1_close")[[1]]$clickElement()
remDr$findElements("id", "buscador_home_zocalo_1x1_close")[[1]]$clickElement()
remDr$findElements("id", "buscador_home_zocalo_1x1_close")[[1]]$clickElement()
pagina <- paste0("https://www.infobae.com/america/buscador/?query=", palabra)
pagina <- read_html(pagina)
remDr$navigate(paste0("https://www.infobae.com/america/buscador/?query=", palabra))
remDr$findElements("id", "buscador_home_zocalo_1x1_close")[[1]]$clickElement()
pagina <- paste0("https://www.infobae.com/america/buscador/?query=", palabra)
pagina <- read_html(pagina)
pagina
resultado <-  remDr$findElements("id", "resultdata")[[1]]
remDr$navigate(paste0("https://www.infobae.com/america/buscador/?query=", palabra))
remDr$findElements("id", "queryly-label")[[1]]$clickElement()
remDr$navigate(paste0("https://www.infobae.com/america/buscador/?query=", palabra))
remDr$findElements("id", "queryly-label")[[1]]$clickElement()
remDr$findElements("id", "hamburger-icon")[[1]]$clickElement()
remDr$findElements("id", "queryly-label")[[1]]$clickElement()
remDr$findElements("id", "hamburger-icon")[[1]]$clickElement()
remDr$findElements("id", "queryly-label")[[1]]$clickElement()
remDr$findElements("id", "search-icon")[[1]]$clickElement()
remDr$findElements("id", "queryly_query")[[1]]$clickElement()
remDr$findElements("id", "queryly_search_header_inner")[[1]]$clickElement()
search_bar <- remDr$findElement(using = "id", value = "queryly_search_header_inner")
remDr$findElements("id", "hamburger-icon")[[1]]$clickElement()
remDr$findElements("id", "search-icon")[[1]]$clickElement()
search_bar <- remDr$findElement(using = "id", value = "queryly_search_header_inner")
# Escribe el texto deseado
search_bar$sendKeysToElement(list("texto de búsqueda"))
search_bar <- remDr$findElement(using = "id", value = "queryly_search_header_inner")
remDr$findElements("id", "hamburger-icon")[[1]]$clickElement()
remDr$navigate(paste0("https://www.infobae.com/america/buscador/?query=", palabra))
remDr$findElements("id", "hamburger-icon")[[1]]$clickElement()
remDr$findElements("id", "search-icon")[[1]]$clickElement()
search_bar <- remDr$findElement(using = "id", value = "queryly_search_input")  # Ajusta el "id" según corresponda
search_bar <- remDr$findElement(using = "id", value = "queryly_query")  # Ajusta el "id" según corresponda
# Envía texto a la barra de búsqueda
search_text <- "tu texto de búsqueda"  # Define el texto que quieres escribir
search_bar$sendKeysToElement(list(search_text))
# Envía texto a la barra de búsqueda
search_text <- "Milei"  # Define el texto que quieres escribir
search_bar$sendKeysToElement(list(search_text))
# Envía texto a la barra de búsqueda
search_text <- "rappi"  # Define el texto que quieres escribir
search_bar$sendKeysToElement(list(search_text))
remDr$navigate("https://www.infobae.com/")
remDr$findElements("id", "hamburger-icon")[[1]]$clickElement()
remDr$findElements("id", "search-icon")[[1]]$clickElement()
search_bar <- remDr$findElement(using = "id", value = "queryly_query")  # Ajusta el "id" según corresponda
search_bar$sendKeysToElement(list("rappi"))
all_links_page_1 <- remDr$findElements("tag name", "a")
show_links_info <- function(link_element, print_output = F) {
text <- as.character(link_element$getElementText())
url <- as.character(link_element$getElementAttribute("href"))
if (print_output) {
cat(paste0(text, ": "),
url,
"\n")
flush.console()
}
c(text, url)
}
saved_list <- lapply(all_links_page_1, show_links_info)
# usa la funcion
links_dataframe <<-  data.frame()
for (i in 1:length(all_links_page_1)) {
links_info <- show_links_info(all_links_page_1[[i]], print = FALSE)
temp_data_frame <- data.frame(index = nrow(links_dataframe) + 1,
text = stringr::str_trim(links_info[1]),
url = links_info[2])
links_dataframe <- rbind(links_dataframe, temp_data_frame)
}
View(links_dataframe)
remDr$navigate("https://www.infobae.com/")
remDr$findElements("id", "hamburger-icon")[[1]]$clickElement()
remDr$navigate("https://www.infobae.com/")
Sys.sleep(5)
remDr$findElements("id", "hamburger-icon")[[1]]$clickElement()
remDr$findElements("id", "search-icon")[[1]]$clickElement()
search_bar <- remDr$findElement(using = "id", value = "queryly_query")  # Ajusta el "id" según corresponda
search_bar$sendKeysToElement(list("web scraping"))
page_source <- remDr$getPageSource()[[1]]
page <- read_html(page_source)
titles <- page %>% html_nodes(".title-class") %>% html_text()  # Cambia ".title-class" al selector correcto
links <- page %>% html_nodes(".title-class a") %>% html_attr("href")  # Cambia ".title-class" al selector correcto
titles <- page %>% html_nodes("div.queryly_item_title") %>% html_text()  # Cambia ".title-class" al selector correcto
titles <- page %>% html_nodes("div.queryly_item_title") %>% html_text() %>%
str_squish()# Cambia ".title-class" al selector correcto
links <- page %>%
html_nodes("div.queryly_item_title") %>%
html_elements("a") %>%
html_attr("href") # Cambia ".title-class" al selector correcto
raiz <- "https://www.infobae.com/"
links_completos <- str_c(raiz, links)
raiz <- "https://www.infobae.com"
links_completos <- str_c(raiz, links)
noticia <-  read_html(links_completos[1])
titulo <- noticia %>%
html_element("h1") %>%
html_text() %>%
str_squish()
cuerpo <- noticia %>%
html_element("p.paragraph") %>%
html_text() %>%
str_squish()
fecha  <- noticia %>%
html_element("span.sharebar-article-date") %>%
html_text() %>%
str_squish()
autor  <- noticia %>%
html_element("span.author-name") %>%
html_text() %>%
str_squish()
source("C:/Users/dpach/OneDrive - sociales.UBA.ar/CURSO_WEB_SCRAPING_HAB/Clase_3_ws/infobae.R", echo=TRUE)
source("C:/Users/dpach/OneDrive - sociales.UBA.ar/CURSO_WEB_SCRAPING_HAB/Clase_3_ws/infobae.R", echo=TRUE)
source("C:/Users/dpach/OneDrive - sociales.UBA.ar/CURSO_WEB_SCRAPING_HAB/Clase_3_ws/infobae.R", echo=TRUE)
source("C:/Users/dpach/OneDrive - sociales.UBA.ar/CURSO_WEB_SCRAPING_HAB/Clase_3_ws/infobae.R", echo=TRUE)
source("C:/Users/dpach/OneDrive - sociales.UBA.ar/CURSO_WEB_SCRAPING_HAB/Clase_3_ws/infobae.R", echo=TRUE)
source("C:/Users/dpach/OneDrive - sociales.UBA.ar/CURSO_WEB_SCRAPING_HAB/Clase_3_ws/infobae.R", echo=TRUE)
source("C:/Users/dpach/OneDrive - sociales.UBA.ar/CURSO_WEB_SCRAPING_HAB/Clase_3_ws/infobae.R", echo=TRUE)
View(noticias)
page_source <- remDr$getPageSource()[[1]]
page <- read_html(page_source)
titles <- page %>% html_nodes("div.queryly_item_title") %>%
html_text() %>%
str_squish()
links <- page %>%
html_nodes("div.queryly_item_title") %>%
html_elements("a") %>%
html_attr("href")
page <- read_html(page_source)
titles <- page %>% html_nodes("div.queryly_item_title") %>%
html_text() %>%
str_squish()
links <- page %>%
html_nodes("div.queryly_item_title") %>%
html_elements("a") %>%
html_attr("href")
source("C:/Users/dpach/OneDrive - sociales.UBA.ar/CURSO_WEB_SCRAPING_HAB/Clase_3_ws/infobae.R", echo=TRUE)
source("C:/Users/dpach/OneDrive - sociales.UBA.ar/CURSO_WEB_SCRAPING_HAB/Clase_3_ws/infobae.R", echo=TRUE)
rd <- rsDriver(browser = "firefox",
chromever = NULL,
port= 4446L)
remDr <- rd[["client"]]
remDr$navigate("https://www.infobae.com/")
remDr$maxWindowSize()
remDr$findElements("id", "onesignal-slidedown-cancel-button")[[1]]$clickElement()
remDr$findElements("id", "hamburger-icon")[[1]]$clickElement()
remDr$findElements("id", "search-icon")[[1]]$clickElement()
search_bar <- remDr$findElement(using = "id", value = "queryly_query")  # Ajusta el "id" según corresponda
search_bar$sendKeysToElement(list("web scraping"))
page_source <- remDr$getPageSource()[[1]]
page <- read_html(page_source)
titles <- page %>% html_nodes("div.queryly_item_title") %>%
html_text() %>%
str_squish()
titles
source("C:/Users/dpach/OneDrive - sociales.UBA.ar/CURSO_WEB_SCRAPING_HAB/Clase_3_ws/infobae.R", echo=TRUE)
source("C:/Users/dpach/OneDrive - sociales.UBA.ar/CURSO_WEB_SCRAPING_HAB/Clase_3_ws/infobae.R", echo=TRUE)
View(noticias)
source("C:/Users/dpach/OneDrive - sociales.UBA.ar/CURSO_WEB_SCRAPING_HAB/Clase_3_ws/infobae.R", echo=TRUE)
View(noticias)
links <- links[!duplicated(links) ]
links <- page %>%
html_nodes("div.queryly_item_title") %>%
html_elements("a") %>%
html_attr("href")
links <- links[!duplicated(links) ]
View(noticias)
source("C:/Users/dpach/OneDrive - sociales.UBA.ar/CURSO_WEB_SCRAPING_HAB/Clase_3_ws/infobae.R", echo=TRUE)
View(noticias)
# Definimos un vector con dos URLs de tweets
urls  <- c("https://x.com/AAS_Sociologia/status/1844666834886283607",
"https://x.com/AAS_Sociologia/status/1844776108778000410",
"https://x.com/AAS_Sociologia/status/1838907832927768645",
"https://x.com/AAS_Sociologia/status/1841819590587822081")
# Seleccionamos la primera URL del vector
url_01 <- urls[1]
# Leemos el contenido HTML de la primera URL usando read_html_live
# Esta función permite interactuar con contenido dinámico (JavaScript)
tweet_01 <- rvest::read_html_live(url_01)
# Extraemos todos los elementos <article> del HTML
# Los tweets suelen estar contenidos en elementos <article>
tweet_01_art <- tweet_01$html_elements("article")
# Extraemos el texto de los elementos <article>
tweet_01_article_text <- rvest::html_text2(tweet_01_art)
# Imprimimos los elementos <article> encontrados
print(tweet_01_art)
# Extraemos el texto de los elementos <article>
tweet_01_article_text <- rvest::html_text2(tweet_01_art)
# Imprimimos el texto extraído de los elementos <article>
print(tweet_01_article_text)
urls[1]
# Extraemos el texto de los elementos <article>
tweet_01_article_text <- rvest::html_text2(tweet_01_art)
# Imprimimos el texto extraído de los elementos <article>
print(tweet_01_article_text)
# Imprimimos el texto del primer elemento <article> encontrado
# Esto debería contener el texto principal del tweet
cat(tweet_01_article_text[1])
# Definimos la URL del perfil de Instagram que queremos analizar
url <- "https://www.instagram.com/sociologiaandaluza/"
# Leemos el contenido HTML de la URL usando read_html_live
# Esta función permite interactuar con contenido dinámico (JavaScript)
ig_aas <- rvest::read_html_live(url)
# Extraemos el texto de todos los elementos <body> y lo imprimimos
# El operador |> es un pipe que pasa el resultado de la izquierda como primer argumento de la función a la derecha
ig_aas$html_elements("body") |> rvest::html_text()
# Extraemos el texto de todos los elementos <body>, seleccionamos el tercer elemento (índice 3) y lo imprimimos
# cat() se usa para imprimir el texto sin comillas y con formato
cat(rvest::html_text(ig_aas$html_elements("body"))[3])
# Cerramos la sesión de Instagram
ig_aas$session$close()
# Instala el paquete TweetScraperR desde GitHub
devtools::install_github("agusnieto77/TweetScraperR")
# Definimos la URL del perfil de Instagram que queremos analizar
url <- "https://www.instagram.com/sociologiaandaluza/"
# Leemos el contenido HTML de la URL usando read_html_live
# Esta función permite interactuar con contenido dinámico (JavaScript)
ig_aas <- rvest::read_html_live(url)
# Extraemos el texto de todos los elementos <body>, seleccionamos el tercer elemento (índice 3) y lo imprimimos
# cat() se usa para imprimir el texto sin comillas y con formato
cat(rvest::html_text(ig_aas$html_elements("body"))[3])
# Definimos la URL del perfil de Instagram que queremos analizar
url <- "https://www.instagram.com/pachedi/"
# Extraemos el texto de todos los elementos <body> y lo imprimimos
# El operador |> es un pipe que pasa el resultado de la izquierda como primer argumento de la función a la derecha
ig_aas$html_elements("body") |> rvest::html_text()
# Cerramos la sesión de Instagram
ig_aas$session$close()
# Definimos la URL del perfil de Instagram que queremos analizar
url <- "https://www.instagram.com/pachedi/"
# Leemos el contenido HTML de la URL usando read_html_live
# Esta función permite interactuar con contenido dinámico (JavaScript)
ig_aas <- rvest::read_html_live(url)
# Extraemos el texto de todos los elementos <body> y lo imprimimos
# El operador |> es un pipe que pasa el resultado de la izquierda como primer argumento de la función a la derecha
ig_aas$html_elements("body") |> rvest::html_text()
# Extraemos el texto de todos los elementos <body>, seleccionamos el tercer elemento (índice 3) y lo imprimimos
# cat() se usa para imprimir el texto sin comillas y con formato
cat(rvest::html_text(ig_aas$html_elements("body"))[3])
knitr::opts_chunk$set(echo = TRUE)
html_discurso <- read_html("https://www.lamoncloa.gob.es/presidente/intervenciones/Paginas/2023/prsp15112023.aspx",encoding = "UTF-8")
library(rvest)
library(stringr)
library(readr)
library(tidyverse)
html_discurso <- read_html("https://www.lamoncloa.gob.es/presidente/intervenciones/Paginas/2023/prsp15112023.aspx",encoding = "UTF-8")
knitr::opts_chunk$set(echo = TRUE)
pagina <- read_html("https://prensa.presidencia.cl/discurso.aspx?id=188237")
discurso <- pagina %>%
html_elements("div.texto-bloque") %>%
html_text()
discurso
discurso <- pagina %>%
html_elements("div.texto-bloque") %>%
html_text() %>%
str_squish()
discurso
write_lines(discurso, "discurso_boric.txt")
